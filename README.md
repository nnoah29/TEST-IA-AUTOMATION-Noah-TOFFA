# FANGA â€” Automatisation IA pour la classification de fichiers

Il s'agit d'une pipeline d'automatisation IA, qui Ã  pour but de classifier et de renommer automatiquement des fichiers d'organisation de la structure FANGA(sociÃ©tÃ© de mobilitÃ© Ã©lectrique 2-roues en CÃ´te d'Ivoire).


---

## Setup et exÃ©cution

### PrÃ©requis

- Python 3.10+
- Une clÃ© API **Gemini** (ou **OpenAI**)

### Installation

```bash
# 1. Cloner le dÃ©pÃ´t
git clone <url_du_repo>
cd Owner avatar
TEST-IA-AUTOMATION-Noah-TOFFA

# 2. CrÃ©er l'environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/macOS
# venv\Scripts\activate   # Windows

# 3. Installer les dÃ©pendances
pip install -r requirements.txt

# 4. Configurer les variables d'environnement
cp .env.example .env
# Ã‰diter .env et renseigner votre clÃ© API
```

### Lancement

```bash
# GÃ©nÃ©rer les fichiers mock (10 fichiers de dÃ©monstration)
python generate_mocks.py

# Lancer le pipeline
python main.py
```

Les fichiers organisÃ©s seront dans `data/fanga_organised/` et le rapport dans `data/fanga_organised/rapport_traitement.json`.



## Architecture

Le projet suis une achitecture modulaire

```
FANGA/
â”œâ”€â”€ main.py                  # Point d'entrÃ©e, configure les logs
â”œâ”€â”€ generate_mocks.py        # GÃ©nÃ¨re les fichiers de test (fanga_inbox/)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py            # ParamÃ¨tres (pydantic-settings, lecture .env)
â”‚   â”œâ”€â”€ models.py            # ModÃ¨les Pydantic (FileAnalysis, ProcessingResult)
â”‚   â”œâ”€â”€ extractors.py        # Extraction de contenu par type de fichier
â”‚   â”œâ”€â”€ ai_engine.py         # Moteurs IA (OpenAI & Gemini, pattern Strategy)
â”‚   â”œâ”€â”€ pipeline.py          # Orchestrateur principal du traitement
â”‚   â””â”€â”€ report.py            # GÃ©nÃ©ration du rapport JSON
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ fanga_inbox/         # Dossier source (fichiers Ã  traiter)
â”‚   â”œâ”€â”€ fanga_organised/     # Dossier de sortie (fichiers classifiÃ©s)
â”‚   â”‚   â”œâ”€â”€ Contrats/
â”‚   â”‚   â”œâ”€â”€ Facture/
â”‚   â”‚   â”œâ”€â”€ Photos/
â”‚   â”‚   â”œâ”€â”€ Rapports/
â”‚   â”‚   â”œâ”€â”€ Export_csv/
â”‚   â”‚   â”œâ”€â”€ Documents_identite/
â”‚   â”‚   â”œâ”€â”€ Maintenance/
â”‚   â”‚   â”œâ”€â”€ Autres/
â”‚   â”‚   â”œâ”€â”€ A_verifier/      # Fichiers dont la confiance est < seuil
â”‚   â”‚   â””â”€â”€ rapport_traitement.json
â”‚   â””â”€â”€ logs/                # Logs persistÃ©s (rotation quotidienne)
â”‚
â””â”€â”€ tests/
    â””â”€â”€ test_pipeline.py     # Tests unitaires (pytest)
```

### RÃ´le de chaque module

| Module | RÃ´le                                                                             |
|---|----------------------------------------------------------------------------------|
| `config.py` | Definition des parametre de configuration externe au code du projet.             |
| `models.py` | DÃ©finition des structures de donnÃ©es.                                            |
| `extractors.py` | Extrait le texte des fichiers selon leur extension (PDF, DOCX, XLSX, CSV).       |
| `ai_engine.py` | La couche IA, implÃ©mente la logique des appels apis des modeles OpenAI et GOOGLE |
| `pipeline.py` | Orchestre chaque fichier : extraction â†’ analyse IA â†’ renommage â†’ dÃ©placement.    |
| `report.py` | GÃ©nÃ¨re le `rapport_traitement.json` final.                                       |

Le code est modulaire et testable. Et chaque module est indÃ©pendant, les uns des autres.

---

## ğŸ§  StratÃ©gie de classification

### Approche : Structured Output LLM + Prompt Engineering

La classification repose sur un appel LLM unique par fichier avec **sortie structurÃ©e Pydantic**, ce qui garantit une rÃ©ponse JSON valide.

**Pipeline par fichier :**

```
Fichier â†’ Extraction contenu (texte/image) â†’ Prompt LLM contextuel â†’ FileAnalysis (catÃ©gorie + score + description) â†’ Renommage â†’ DÃ©placement
```

**Prompt system :** Le prompt dÃ©crit les 8 catÃ©gories mÃ©tier de FANGA avec leurs critÃ¨res (ex: "Maintenance : bugs applicatifs, captures d'Ã©cran d'erreurs, pannes, rÃ©parations, entretien matÃ©riel/flotte"). Cela ancre la classification dans le contexte mÃ©tier rÃ©el.

**Score de confiance :** Le modÃ¨le retourne un score `[0, 1]`. Si `confiance < CONFIDENCE_THRESHOLD` (dÃ©faut : 0.7), le fichier est dÃ©placÃ© dans `A_verifier/` avec une note explicative, et le pipeline continue sans interruption.

**Support Vision :** Les fichiers image (`.jpg`, `.png`, `.webp`) sont encodÃ©s en base64 et envoyÃ©s directement au modÃ¨le multimodal (Gemini Vision ou GPT-4o Vision), permettant une classification basÃ©e sur le contenu visuel rÃ©el.

**Fournisseurs supportÃ©s :**

| Provider | Variable `.env` | ModÃ¨le par dÃ©faut |
|---|---|---|
| Google Gemini | `AI_PROVIDER=gemini` | `gemini-3-flash-preview` |
| OpenAI | `AI_PROVIDER=openai` | `gpt-4o` |

**Robustesse :** Chaque appel LLM est relancÃ© jusqu'Ã  3 fois avec backoff exponentiel (`tenacity`). Toute exception par fichier est captÃ©e : le fichier est dÃ©placÃ© dans `Erreurs/` et le pipeline continue.

---

## AmÃ©liorations envisagÃ©es Ã  moyen terme

1. **Surveillance de dossier en temps rÃ©el** : Remplacer le scan ponctuel par un watcher  pour traiter les fichiers dÃ¨s leur arrivÃ©e dans `fanga_inbox/`.
2. **DÃ©tection de doublons** : Calcul de hash (SHA-256) de chaque fichier pour Ã©viter les retraitements et signaler les copies identiques avec des noms diffÃ©rents.
3. **Interface de rÃ©vision** : Petite UI (Streamlit ou web) pour qu'un opÃ©rateur traite rapidement les fichiers dans `A_verifier/` : voir le fichier, valider ou corriger la catÃ©gorie proposÃ©e.
4. **Notifications** : Envoi de rÃ©sumÃ© par email ou Slack Ã  la fin de chaque batch (nombre de fichiers traitÃ©s, erreurs, fichiers Ã  vÃ©rifier).
5. **File d'attente** : Pour des volumes importants, placer les fichiers dans une queue (Celery + Redis) pour un traitement asynchrone et horizontal.
6. **Fine-tuning / few-shot** : Enrichir le prompt avec des exemples rÃ©els issus du contexte FANGA pour amÃ©liorer la prÃ©cision sur les cas ambigus.

---

## Question finale â€” Passage Ã  l'Ã©chelle

> FANGA prÃ©voit de traiter automatiquement des milliers de fichiers par jour provenant de dizaines d'agences partenaires en CÃ´te d'Ivoire, avec des noms de fichiers parfois incohÃ©rents et des formats variÃ©s. Comment feriez-vous Ã©voluer votre solution pour rÃ©pondre Ã  ce volume, garantir la fiabilitÃ© de la classification et intÃ©grer une boucle de correction humaine lorsque le modÃ¨le se trompe ?

L'architecture actuelle est synchrone et mono-thread. Pour des milliers de fichiers/jour, je migrerais vers une **architecture asynchrone orientÃ©e Ã©vÃ©nements** :

- **Ingestion** : Chaque agence partenaire dÃ©pose ses fichiers dans un bucket S3 dÃ©diÃ© (ou un Google Drive partagÃ©). Un Ã©vÃ©nement `ObjectCreated` dÃ©clenche un message dans une **queue** (SQS, RabbitMQ ou Redis Streams).
- **Workers parallÃ¨les** : Plusieurs workers Python (`Celery` ou `asyncio`) consomment la queue, chacun traitant un fichier indÃ©pendamment.
- **Gestion des doublons** : Chaque fichier est identifiÃ© par son hash SHA-256. Si un fichier dÃ©jÃ  traitÃ© rÃ©apparaÃ®t (mÃªme contenu, nom diffÃ©rent), il est dÃ©tectÃ© comme doublon sans appel LLM.
- **Stratification par confiance** : Garder le seuil Ã  70% mais affiner avec des seuils par catÃ©gorie (ex: Documents_identite Ã  85% car plus sensible).
- **Consensus multi-modÃ¨les** : Pour les cas ambigus (60â€“80%), lancer deux modÃ¨les (ex: Gemini + GPT-4o) et ne valider que si les deux s'accordent. Sinon â†’ rÃ©vision humaine.
- **Interface de rÃ©vision** : Une UI simple (admin web) affiche les fichiers de `A_verifier/` avec leur aperÃ§u, la catÃ©gorie proposÃ©e par le modÃ¨le et son score. L'opÃ©rateur peut valider, corriger ou rejeter en un clic.
